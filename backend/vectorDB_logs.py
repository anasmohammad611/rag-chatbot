import json
import logging
import os
from typing import List, Dict, Tuple

import chromadb
from sentence_transformers import SentenceTransformer

# Enhanced logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ChromaVectorDB:
    def __init__(self, collection_name: str = "angelone_support", persist_dir: str = "./vector_db"):
        """Initialize ChromaDB vector database"""
        logger.info("ğŸš€ [INIT] Starting ChromaVectorDB initialization...")
        logger.info(f"ğŸš€ [INIT] Collection name: {collection_name}")
        logger.info(f"ğŸš€ [INIT] Persist directory: {persist_dir}")

        self.collection_name = collection_name
        self.persist_dir = persist_dir

        # Create directory
        logger.info("ğŸ“ [INIT] Creating persist directory...")
        os.makedirs(persist_dir, exist_ok=True)
        logger.info(f"ğŸ“ [INIT] âœ… Directory created/exists: {persist_dir}")

        # Initialize ChromaDB client with persistence
        logger.info("ğŸ”Œ [INIT] Initializing ChromaDB PersistentClient...")
        self.client = chromadb.PersistentClient(path=persist_dir)
        logger.info("ğŸ”Œ [INIT] âœ… ChromaDB client initialized successfully")

        # Initialize embedding model
        logger.info("ğŸ§  [INIT] Loading SentenceTransformer model (all-MiniLM-L6-v2)...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        logger.info("ğŸ§  [INIT] âœ… Embedding model loaded successfully")
        logger.info(
            f"ğŸ§  [INIT] Model creates {self.embedding_model.get_sentence_embedding_dimension()}-dimensional vectors")

        # Get or create collection
        logger.info(f"ğŸ—ƒï¸  [INIT] Attempting to load existing collection '{collection_name}'...")
        try:
            self.collection = self.client.get_collection(
                name=collection_name,
                embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name="all-MiniLM-L6-v2"
                )
            )
            logger.info(f"ğŸ—ƒï¸  [INIT] âœ… Loaded existing collection: {collection_name}")
        except Exception as e:
            logger.info(f"ğŸ—ƒï¸  [INIT] Collection doesn't exist, creating new one. Error: {e}")
            self.collection = self.client.create_collection(
                name=collection_name,
                embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name="all-MiniLM-L6-v2"
                )
            )
            logger.info(f"ğŸ—ƒï¸  [INIT] âœ… Created new collection: {collection_name}")

        current_count = self.collection.count()
        logger.info(f"ğŸ“Š [INIT] Collection currently has {current_count} documents")
        logger.info("ğŸš€ [INIT] âœ… ChromaVectorDB initialization complete!")

    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks"""
        logger.info(f"âœ‚ï¸  [CHUNK] Starting text chunking...")
        logger.info(f"âœ‚ï¸  [CHUNK] Input text length: {len(text)} characters")
        logger.info(f"âœ‚ï¸  [CHUNK] Chunk size: {chunk_size} words, Overlap: {overlap} words")

        words = text.split()
        logger.info(f"âœ‚ï¸  [CHUNK] Split into {len(words)} words")

        chunks = []
        step_size = chunk_size - overlap
        logger.info(f"âœ‚ï¸  [CHUNK] Step size (chunk_size - overlap): {step_size} words")

        for i in range(0, len(words), step_size):
            chunk_start = i
            chunk_end = min(i + chunk_size, len(words))
            chunk = ' '.join(words[chunk_start:chunk_end])

            logger.info(f"âœ‚ï¸  [CHUNK] Processing chunk {len(chunks) + 1}: words {chunk_start}-{chunk_end}")
            logger.info(f"âœ‚ï¸  [CHUNK] Chunk length: {len(chunk)} characters, {len(chunk.split())} words")

            if len(chunk.strip()) > 50:  # Only keep substantial chunks
                chunks.append(chunk.strip())
                logger.info(f"âœ‚ï¸  [CHUNK] âœ… Chunk {len(chunks)} saved (substantial content)")
                logger.info(f"âœ‚ï¸  [CHUNK] Preview: {chunk[:100]}...")
            else:
                logger.info(f"âœ‚ï¸  [CHUNK] âŒ Chunk skipped (too short: {len(chunk.strip())} chars)")

        logger.info(f"âœ‚ï¸  [CHUNK] âœ… Chunking complete! Created {len(chunks)} chunks from {len(words)} words")
        return chunks

    def process_documents(self, documents: List[Dict]) -> Tuple[List[str], List[str], List[Dict]]:
        """Process scraped documents into chunks with metadata"""
        logger.info(f"ğŸ“„ [PROCESS] Starting document processing...")
        logger.info(f"ğŸ“„ [PROCESS] Total documents to process: {len(documents)}")

        all_texts = []
        all_ids = []
        all_metadata = []

        for doc_idx, doc in enumerate(documents):
            logger.info(f"ğŸ“„ [PROCESS] Processing document {doc_idx + 1}/{len(documents)}")
            logger.info(f"ğŸ“„ [PROCESS] Document title: {doc.get('title', 'No title')}")
            logger.info(f"ğŸ“„ [PROCESS] Document URL: {doc.get('url', 'No URL')}")
            logger.info(f"ğŸ“„ [PROCESS] Document content length: {len(doc.get('content', ''))} characters")

            # Create chunks from the document content
            logger.info(f"ğŸ“„ [PROCESS] Chunking document {doc_idx + 1}...")
            chunks = self.chunk_text(doc['content'])
            logger.info(f"ğŸ“„ [PROCESS] Document {doc_idx + 1} created {len(chunks)} chunks")

            for chunk_idx, chunk in enumerate(chunks):
                logger.info(f"ğŸ“„ [PROCESS] Processing chunk {chunk_idx + 1}/{len(chunks)} from doc {doc_idx + 1}")

                # Create unique ID
                chunk_id = f"doc_{doc_idx}_chunk_{chunk_idx}"
                logger.info(f"ğŸ“„ [PROCESS] Generated chunk ID: {chunk_id}")

                # Prepare metadata
                metadata = {
                    'source_url': doc['url'],
                    'title': doc['title'],
                    'chunk_index': chunk_idx,
                    'total_chunks': len(chunks),
                    'word_count': len(chunk.split()),
                    'doc_index': doc_idx
                }
                logger.info(f"ğŸ“„ [PROCESS] Chunk metadata: {metadata}")

                all_texts.append(chunk)
                all_ids.append(chunk_id)
                all_metadata.append(metadata)

                logger.info(f"ğŸ“„ [PROCESS] âœ… Chunk {chunk_id} added to processing queue")
                logger.info(f"ğŸ“„ [PROCESS] Current total chunks processed: {len(all_texts)}")

        logger.info(f"ğŸ“„ [PROCESS] âœ… Document processing complete!")
        logger.info(f"ğŸ“„ [PROCESS] Final stats: {len(all_texts)} chunks from {len(documents)} documents")
        logger.info(f"ğŸ“„ [PROCESS] Average chunks per document: {len(all_texts) / len(documents):.1f}")
        return all_texts, all_ids, all_metadata

    def build_index(self, documents: List[Dict]):
        """Build ChromaDB index from documents"""
        logger.info("ğŸ—ï¸  [BUILD] Starting ChromaDB index build...")
        logger.info(f"ğŸ—ï¸  [BUILD] Input: {len(documents)} documents")

        # Check if collection already has data
        current_count = self.collection.count()
        logger.info(f"ğŸ—ï¸  [BUILD] Current collection count: {current_count}")

        if current_count > 0:
            logger.info("ğŸ—ï¸  [BUILD] Collection has existing data, clearing...")
            # Clear existing data
            existing_data = self.collection.get()
            existing_ids = existing_data['ids']
            logger.info(f"ğŸ—ï¸  [BUILD] Found {len(existing_ids)} existing items to delete")

            if existing_ids:
                logger.info("ğŸ—ï¸  [BUILD] Deleting existing data...")
                self.collection.delete(ids=existing_ids)
                logger.info("ğŸ—ï¸  [BUILD] âœ… Existing data cleared")
            else:
                logger.info("ğŸ—ï¸  [BUILD] No existing IDs found to delete")

        # Process documents into chunks
        logger.info("ğŸ—ï¸  [BUILD] Processing documents into chunks...")
        texts, ids, metadata = self.process_documents(documents)
        logger.info(f"ğŸ—ï¸  [BUILD] Ready to insert {len(texts)} chunks into ChromaDB")

        # Add documents to collection in batches
        batch_size = 100
        total_batches = (len(texts) - 1) // batch_size + 1
        logger.info(
            f"ğŸ—ï¸  [BUILD] Batch insertion starting: {batch_size} items per batch, {total_batches} total batches")

        for batch_num, i in enumerate(range(0, len(texts), batch_size), 1):
            logger.info(f"ğŸ—ï¸  [BUILD] Processing batch {batch_num}/{total_batches}")

            batch_texts = texts[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            batch_metadata = metadata[i:i + batch_size]

            logger.info(f"ğŸ—ï¸  [BUILD] Batch {batch_num} contains {len(batch_texts)} items")
            logger.info(f"ğŸ—ï¸  [BUILD] Batch range: items {i} to {min(i + batch_size - 1, len(texts) - 1)}")
            logger.info(f"ğŸ—ï¸  [BUILD] Sample IDs in this batch: {batch_ids[:3]}...")

            logger.info(f"ğŸ—ï¸  [BUILD] Inserting batch {batch_num} into ChromaDB...")
            self.collection.add(
                documents=batch_texts,
                ids=batch_ids,
                metadatas=batch_metadata
            )

            # Verify insertion
            current_count_after_batch = self.collection.count()
            logger.info(f"ğŸ—ï¸  [BUILD] âœ… Batch {batch_num} inserted successfully")
            logger.info(f"ğŸ—ï¸  [BUILD] Collection count after batch {batch_num}: {current_count_after_batch}")

            # Sample a few items to verify they were inserted correctly
            if batch_num == 1:  # Only do this for first batch to avoid spam
                logger.info("ğŸ—ï¸  [BUILD] Verifying insertion by fetching sample items...")
                sample_fetch = self.collection.get(ids=batch_ids[:2], include=['documents', 'metadatas'])
                for idx, (sample_id, sample_doc, sample_meta) in enumerate(zip(
                        sample_fetch['ids'], sample_fetch['documents'], sample_fetch['metadatas']
                )):
                    logger.info(f"ğŸ—ï¸  [BUILD] Sample item {idx + 1}:")
                    logger.info(f"ğŸ—ï¸  [BUILD]   ID: {sample_id}")
                    logger.info(f"ğŸ—ï¸  [BUILD]   Metadata: {sample_meta}")
                    logger.info(f"ğŸ—ï¸  [BUILD]   Content preview: {sample_doc[:100]}...")

        final_count = self.collection.count()
        logger.info(f"ğŸ—ï¸  [BUILD] âœ… Index build complete!")
        logger.info(f"ğŸ—ï¸  [BUILD] Final collection count: {final_count}")
        logger.info(f"ğŸ—ï¸  [BUILD] Expected count: {len(texts)}")

        if final_count == len(texts):
            logger.info("ğŸ—ï¸  [BUILD] âœ… Count verification passed - all items inserted successfully")
        else:
            logger.warning(f"ğŸ—ï¸  [BUILD] âš ï¸  Count mismatch! Expected {len(texts)}, got {final_count}")

    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.3) -> List[Dict]:
        """Search for similar chunks"""
        logger.info(f"ğŸ” [SEARCH] Starting search...")
        logger.info(f"ğŸ” [SEARCH] Query: '{query}'")
        logger.info(f"ğŸ” [SEARCH] Top K: {top_k}")
        logger.info(f"ğŸ” [SEARCH] Min similarity threshold: {min_similarity}")

        try:
            logger.info("ğŸ” [SEARCH] Executing ChromaDB query...")
            search_results_count = top_k * 2
            logger.info(f"ğŸ” [SEARCH] Requesting {search_results_count} results for filtering")

            results = self.collection.query(
                query_texts=[query],
                n_results=search_results_count,
                include=['documents', 'metadatas', 'distances']
            )

            logger.info("ğŸ” [SEARCH] âœ… ChromaDB query completed")
            logger.info(
                f"ğŸ” [SEARCH] Raw results count: {len(results['documents'][0]) if results['documents'][0] else 0}")

            # Format results
            formatted_results = []
            if results['documents'][0]:  # Check if we have results
                logger.info("ğŸ” [SEARCH] Processing and filtering results...")

                for i in range(len(results['documents'][0])):
                    # ChromaDB returns cosine distance, convert to similarity
                    distance = results['distances'][0][i]
                    similarity = 1 - distance if distance <= 1 else 1 / (1 + distance)

                    logger.info(f"ğŸ” [SEARCH] Result {i + 1}:")
                    logger.info(f"ğŸ” [SEARCH]   Distance: {distance:.4f}")
                    logger.info(f"ğŸ” [SEARCH]   Similarity: {similarity:.4f}")
                    logger.info(f"ğŸ” [SEARCH]   Title: {results['metadatas'][0][i].get('title', 'No title')}")
                    logger.info(f"ğŸ” [SEARCH]   Content preview: {results['documents'][0][i][:100]}...")

                    # Only include results above similarity threshold
                    if similarity >= min_similarity:
                        result = {
                            'text': results['documents'][0][i],
                            'metadata': results['metadatas'][0][i],
                            'similarity_score': similarity,
                            'distance': distance
                        }
                        formatted_results.append(result)
                        logger.info(
                            f"ğŸ” [SEARCH]   âœ… Result {i + 1} passed threshold (similarity: {similarity:.4f} >= {min_similarity})")
                    else:
                        logger.info(
                            f"ğŸ” [SEARCH]   âŒ Result {i + 1} filtered out (similarity: {similarity:.4f} < {min_similarity})")

                # Sort by similarity and limit to top_k
                logger.info(f"ğŸ” [SEARCH] Sorting {len(formatted_results)} results by similarity...")
                formatted_results.sort(key=lambda x: x['similarity_score'], reverse=True)

                original_count = len(formatted_results)
                formatted_results = formatted_results[:top_k]
                final_count = len(formatted_results)

                logger.info(f"ğŸ” [SEARCH] Limited results from {original_count} to top {final_count}")

                # Log final results summary
                logger.info("ğŸ” [SEARCH] Final results summary:")
                for idx, result in enumerate(formatted_results, 1):
                    logger.info(
                        f"ğŸ” [SEARCH]   {idx}. Score: {result['similarity_score']:.4f} - {result['metadata'].get('title', 'No title')}")
            else:
                logger.info("ğŸ” [SEARCH] No results returned from ChromaDB")

            logger.info(f"ğŸ” [SEARCH] âœ… Search complete! Returning {len(formatted_results)} results")
            return formatted_results

        except Exception as e:
            logger.error(f"ğŸ” [SEARCH] âŒ Error during search: {e}")
            logger.error(f"ğŸ” [SEARCH] Query that failed: '{query}'")
            return []

    def get_stats(self) -> Dict:
        """Get database statistics"""
        logger.info("ğŸ“Š [STATS] Generating database statistics...")
        try:
            count = self.collection.count()
            stats = {
                "status": "ready",
                "total_chunks": count,
                "collection_name": self.collection_name,
                "persist_dir": self.persist_dir
            }
            logger.info(f"ğŸ“Š [STATS] âœ… Stats generated successfully: {stats}")
            return stats
        except Exception as e:
            error_stats = {
                "status": "error",
                "error": str(e)
            }
            logger.error(f"ğŸ“Š [STATS] âŒ Error generating stats: {error_stats}")
            return error_stats

    def delete_collection(self):
        """Delete the collection (for testing)"""
        logger.info(f"ğŸ—‘ï¸  [DELETE] Attempting to delete collection: {self.collection_name}")
        try:
            self.client.delete_collection(self.collection_name)
            logger.info(f"ğŸ—‘ï¸  [DELETE] âœ… Collection deleted successfully: {self.collection_name}")
        except Exception as e:
            logger.warning(f"ğŸ—‘ï¸  [DELETE] âš ï¸  Could not delete collection: {e}")


# Test and build the vector database
def build_vector_db():
    """Build vector database from scraped data"""
    logger.info("ğŸš€ [MAIN] Starting build_vector_db function...")

    # Load scraped data
    json_file = 'angelone_support_data.json'
    logger.info(f"ğŸ“– [MAIN] Loading scraped data from {json_file}...")

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        logger.info(f"ğŸ“– [MAIN] âœ… Successfully loaded {len(documents)} documents")

        # Log sample document structure
        if documents:
            sample_doc = documents[0]
            logger.info("ğŸ“– [MAIN] Sample document structure:")
            logger.info(f"ğŸ“– [MAIN]   Keys: {list(sample_doc.keys())}")
            logger.info(f"ğŸ“– [MAIN]   Title: {sample_doc.get('title', 'No title')}")
            logger.info(f"ğŸ“– [MAIN]   URL: {sample_doc.get('url', 'No URL')}")
            logger.info(f"ğŸ“– [MAIN]   Content length: {len(sample_doc.get('content', ''))} chars")
    except FileNotFoundError:
        logger.error(f"ğŸ“– [MAIN] âŒ File not found: {json_file}")
        return None
    except Exception as e:
        logger.error(f"ğŸ“– [MAIN] âŒ Error loading file: {e}")
        return None

    # Initialize vector DB
    logger.info("ğŸ”§ [MAIN] Initializing ChromaVectorDB...")
    vector_db = ChromaVectorDB()

    # Build index
    logger.info("ğŸ—ï¸  [MAIN] Building vector database index...")
    vector_db.build_index(documents)

    # Test search
    test_queries = [
        "How to add funds to my account?",
        "How to operate a brain surgery?"
    ]

    logger.info("ğŸ§ª [MAIN] Starting search tests...")
    print("\nğŸ” TESTING SEARCH:")
    for query_idx, query in enumerate(test_queries, 1):
        logger.info(f"ğŸ§ª [MAIN] Test query {query_idx}/{len(test_queries)}: '{query}'")
        results = vector_db.search(query, top_k=3)
        print(f"\nQuery: '{query}'")

        if results:
            logger.info(f"ğŸ§ª [MAIN] Query '{query}' returned {len(results)} results")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result['metadata']['title']}")
                print(f"     Score: {result['similarity_score']:.3f}")
                print(f"     Preview: {result['text'][:100]}...")
        else:
            logger.info(f"ğŸ§ª [MAIN] Query '{query}' returned no results")
            print("  No results found")

    # Print stats
    logger.info("ğŸ“Š [MAIN] Generating final database statistics...")
    stats = vector_db.get_stats()
    print(f"\nğŸ“Š DATABASE STATS:")
    print(f"Status: {stats['status']}")
    print(f"Total chunks: {stats['total_chunks']}")
    print(f"Collection: {stats['collection_name']}")

    logger.info("ğŸš€ [MAIN] âœ… build_vector_db function completed successfully")
    return vector_db


if __name__ == "__main__":
    logger.info("ğŸŒŸ [ENTRY] Script started - running build_vector_db")
    result = build_vector_db()
    if result:
        logger.info("ğŸŒŸ [ENTRY] âœ… Script completed successfully")
    else:
        logger.error("ğŸŒŸ [ENTRY] âŒ Script failed")
